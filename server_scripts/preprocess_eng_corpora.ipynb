{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocess_eng_corpora.ipynb","provenance":[{"file_id":"1EMGz9atX_0rV906ynWj1lHGXhXuM9MvW","timestamp":1583418064988},{"file_id":"1jmoUcOE7lOwp-fqj6wjtHZpe92ob3Xom","timestamp":1579171866963},{"file_id":"https://github.com/adityak6798/Transformers-For-Negation-and-Speculation/blob/master/Transformers_for_Negation_and_Speculation.ipynb","timestamp":1578674765715}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"F0sTD6pVAWK-"},"source":["## This script preprocesses corpora in the format of Sherlock, SFU, and Bioscope. The Data class is taken from NegBERT by Khandelwal et al. and is adjusted to extract three vectors per sentence: tokens, cue labels, scope labels. The script puts them in a nested list and then writes them to json files.\n"]},{"cell_type":"code","metadata":{"id":"mGTg8YDGGbCy"},"source":["# Imports needed\n","import os, re, torch, html, json, math, random"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tq0WavgDIGJy"},"source":["Upload the datasets to Google Drive. \n","This allows access to your Google Drive from this notebook."]},{"cell_type":"code","metadata":{"id":"h7_IpJcCHOGK","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1594213101565,"user_tz":-120,"elapsed":23944,"user":{"displayName":"Anastassia Shaitarova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64","userId":"01489516284222648289"}},"outputId":"4f69b9b9-45d8-4bb7-91fd-58a683a00aaf"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bWttlhG5e52P"},"source":["TASK = 'negation'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0BuBr5yWJ1Cj"},"source":["## Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"Xe1kG3IZKBrD"},"source":["class Data:\n","    def __init__(self, file, dataset_name = 'sfu', frac_no_cue_sents = 1.0):\n","        self.the_big_data_list = []\n","        '''\n","        file: The path of the data file.\n","        dataset_name: The name of the dataset to be preprocessed. Values supported: sfu, bioscope, starsem.\n","        frac_no_cue_sents: The fraction of sentences to be included in the data object which have no negation/speculation cues.\n","        '''\n","        def starsem(f_path, cue_sents_only=False, frac_no_cue_sents = 1.0):\n","            raw_data = open(f_path)\n","            sentence = []\n","            labels = []\n","            label = []\n","            scope_sents = []\n","            data_scope = []\n","            scope = []\n","            scope_cues = []\n","            # list of lists of all sentences\n","            data = []\n","            cue_only_data = []\n","            \n","            for line in raw_data:\n","                label = []\n","                sentence = []\n","                tokens = line.strip().split()\n","\n","                # go through sentneces with NO NEGATION\n","                if len(tokens)==8: #This line has no cues\n","                        # append the word\n","                        sentence.append(tokens[3])\n","                        label.append(3) #Not a cue\n","                        for line in raw_data:\n","                            tokens = line.strip().split()\n","                            if len(tokens)==0:\n","                                break\n","                            else:\n","                                sentence.append(tokens[3])\n","                                label.append(3)\n","                        cue_only_data.append([sentence, label])\n","                        \n","\n","               # deal with sentences WITH NEGATION     \n","                else: #The line has 1 or more cues\n","                    num_cues = (len(tokens)-7)//3\n","                    #cue_count+=num_cues\n","                    scope = [[] for i in range(num_cues)]\n","                    label = [[],[]] #First list is the real labels, second list is to modify if it is a multi-word cue.\n","                    label[0].append(3) #Generally not a cue, if it is will be set ahead.\n","                    label[1].append(-1) #Since not a cue, for now.\n","                    for i in range(num_cues):\n","                        # collect labels for CUES\n","                        if tokens[7+3*i] != '_': #Cue field is active\n","                            if tokens[8+3*i] != '_': #Check for affix\n","                                label[0][-1] = 0 #Affix\n","                                # this list is not defined or used anywhere!\n","                                affix_list.append(tokens[7+3*i])\n","                                label[1][-1] = i #Cue number\n","                                #sentence.append(tokens[7+3*i])\n","                                #new_word = '##'+tokens[8+3*i]\n","                            else:\n","                                label[0][-1] = 1 #Maybe a normal or multiword cue. The next few words will determine which.\n","                                label[1][-1] = i #Which cue field, for multiword cue altering.\n","\n","                        # collect labels for tokens in the SCOPE     \n","                        if tokens[8+3*i] != '_':\n","                            scope[i].append(1)\n","\n","                        # labels for non-scope tokens\n","                        else:\n","                            scope[i].append(0)\n","\n","                    # append the word\n","                    sentence.append(tokens[3])\n","                    for line in raw_data:\n","                        tokens = line.strip().split()\n","                        if len(tokens)==0:\n","                            break\n","                        else:\n","                            sentence.append(tokens[3])\n","                            label[0].append(3) #Generally not a cue, if it is will be set ahead.\n","                            label[1].append(-1) #Since not a cue, for now.   \n","                            for i in range(num_cues):\n","                                if tokens[7+3*i] != '_': #Cue field is active\n","                                    if tokens[8+3*i] != '_': #Check for affix\n","                                        label[0][-1] = 0 #Affix\n","                                        label[1][-1] = i #Cue number\n","                                    else:\n","                                        label[0][-1] = 1 #Maybe a normal or multiword cue. The next few words will determine which.\n","                                        label[1][-1] = i #Which cue field, for multiword cue altering.\n","                                if tokens[8+3*i] != '_':\n","                                    scope[i].append(1)\n","                                else:\n","                                    scope[i].append(0)\n","\n","\n","                    # fix multiword cues\n","                    for i in range(num_cues):\n","                        indices = [index for index,j in enumerate(label[1]) if i==j]\n","                        count = len(indices)\n","                        if count>1:\n","                            for j in indices:\n","                                label[0][j] = 2\n","                    for i in range(num_cues):\n","                        sc = []\n","                        for a,b in zip(label[0],label[1]):\n","                            if i==b:\n","                                sc.append(a)\n","                            else:\n","                                sc.append(3)\n","                        scope_cues.append(sc)\n","                        scope_sents.append(sentence)\n","                        data_scope.append(scope[i])\n","                    labels.append(label[0])\n","                    data.append(sentence)\n","            cue_only_samples = random.sample(cue_only_data, k=int(frac_no_cue_sents*len(cue_only_data)))\n","            cue_only_sents = [i[0] for i in cue_only_samples]\n","            cue_only_cues = [i[1] for i in cue_only_samples]\n","            starsem_cues = (data+cue_only_sents,labels+cue_only_cues)\n","            starsem_scopes = (scope_sents, scope_cues, data_scope)\n","            return [starsem_cues, starsem_scopes]\n","\n","            \n","        def bioscope(f_path, cue_sents_only=False, frac_no_cue_sents = 1.0):\n","            file = open(f_path, encoding = 'utf-8')\n","            sentences = []\n","            for s in file:\n","                sentences+=re.split(\"(<.*?>)\", html.unescape(s))\n","            cue_sentence = []\n","            cue_cues = []\n","            cue_only_data = []\n","            scope_cues = []\n","            scope_scopes = []\n","            scope_sentence = []\n","            sentence = []\n","            cue = {}\n","            scope = {}\n","            in_scope = []\n","            in_cue = []\n","            word_num = 0\n","            c_idx = []\n","            s_idx = []\n","            in_sentence = 0\n","            for token in sentences:\n","                if token == '':\n","                    continue\n","                elif '<sentence' in token:\n","                    in_sentence = 1\n","                elif '<cue' in token:\n","                    if TASK in token:\n","                        in_cue.append(str(re.split('(ref=\".*?\")',token)[1][4:]))\n","                        c_idx.append(str(re.split('(ref=\".*?\")',token)[1][4:]))\n","                        cue[c_idx[-1]] = []\n","                elif '</cue' in token:\n","                    in_cue = in_cue[:-1]\n","                elif '<xcope' in token:\n","                    #print(re.split('(id=\".*?\")',token)[1][3:])\n","                    in_scope.append(str(re.split('(id=\".*?\")',token)[1][3:]))\n","                    s_idx.append(str(re.split('(id=\".*?\")',token)[1][3:]))\n","                    scope[s_idx[-1]] = []\n","                elif '</xcope' in token:\n","                    in_scope = in_scope[:-1]\n","                elif '</sentence' in token:\n","                    #print(cue, scope)\n","                    if len(cue.keys())==0:\n","                        cue_only_data.append([sentence, [3]*len(sentence)])\n","                    else:\n","                        cue_sentence.append(sentence)\n","                        cue_cues.append([3]*len(sentence))\n","                        for i in cue.keys():\n","                            scope_sentence.append(sentence)\n","                            scope_cues.append([3]*len(sentence))\n","                            if len(cue[i])==1:\n","                                cue_cues[-1][cue[i][0]] = 1\n","                                scope_cues[-1][cue[i][0]] = 1\n","                            else:\n","                                for c in cue[i]:\n","                                    cue_cues[-1][c] = 2\n","                                    scope_cues[-1][c] = 2\n","                            scope_scopes.append([0]*len(sentence))\n","\n","                            if i in scope.keys():\n","                                for s in scope[i]:\n","                                    scope_scopes[-1][s] = 1\n","\n","                    sentence = []\n","                    cue = {}\n","                    scope = {}\n","                    in_scope = []\n","                    in_cue = []\n","                    word_num = 0\n","                    in_sentence = 0\n","                    c_idx = []\n","                    s_idx = []\n","                elif '<' not in token:\n","                    if in_sentence==1:\n","                        words = token.split()\n","                        sentence+=words\n","                        if len(in_cue)!=0:\n","                            for i in in_cue:\n","                                cue[i]+=[word_num+i for i in range(len(words))]\n","                        elif len(in_scope)!=0:\n","                            for i in in_scope:\n","                                scope[i]+=[word_num+i for i in range(len(words))]\n","                        word_num+=len(words)\n","            cue_only_samples = random.sample(cue_only_data, k=int(frac_no_cue_sents*len(cue_only_data)))\n","            cue_only_sents = [i[0] for i in cue_only_samples]\n","            cue_only_cues = [i[1] for i in cue_only_samples]\n","            return [(cue_sentence+cue_only_sents, cue_cues+cue_only_cues),(scope_sentence, scope_cues, scope_scopes)]\n","        \n","        def sfu_review(f_path, cue_sents_only=False, frac_no_cue_sents = 1.0):\n","            file = open(f_path, encoding = 'utf-8')\n","            sentences = []\n","            for s in file:\n","                sentences+=re.split(\"(<.*?>)\", html.unescape(s))\n","            cue_sentence = []\n","            cue_cues = []\n","            scope_cues = []\n","            scope_scopes = []\n","            scope_sentence = []\n","            sentence = []\n","            cue = {}\n","            scope = {}\n","            in_scope = []\n","            in_cue = []\n","            word_num = 0\n","            c_idx = []\n","            cue_only_data = []\n","            s_idx = []\n","            in_word = 0\n","            for token in sentences:\n","                if token == '':\n","                    continue\n","                elif token == '<W>':\n","                    in_word = 1\n","                elif token == '</W>':\n","                    in_word = 0\n","                    word_num += 1\n","                elif '<cue' in token:\n","                    if TASK in token:\n","                        in_cue.append(int(re.split('(ID=\".*?\")',token)[1][4:-1]))\n","                        c_idx.append(int(re.split('(ID=\".*?\")',token)[1][4:-1]))\n","                        cue[c_idx[-1]] = []\n","                elif '</cue' in token:\n","                    in_cue = in_cue[:-1]\n","                elif '<xcope' in token:\n","                    continue\n","                elif '</xcope' in token:\n","                    in_scope = in_scope[:-1]\n","                elif '<ref' in token:\n","                    in_scope.append([int(i) for i in re.split('(SRC=\".*?\")',token)[1][5:-1].split(' ')])\n","                    s_idx.append([int(i) for i in re.split('(SRC=\".*?\")',token)[1][5:-1].split(' ')])\n","                    for i in s_idx[-1]:\n","                        scope[i] = []\n","                elif '</SENTENCE' in token:\n","                    if len(cue.keys())==0:\n","                        cue_only_data.append([sentence, [3]*len(sentence)])\n","                    else:\n","                        cue_sentence.append(sentence)\n","                        cue_cues.append([3]*len(sentence))\n","                        for i in cue.keys():\n","                            scope_sentence.append(sentence)\n","                            scope_cues.append([3]*len(sentence))\n","                            if len(cue[i])==1:\n","                                cue_cues[-1][cue[i][0]] = 1\n","                                scope_cues[-1][cue[i][0]] = 1\n","                            else:\n","                                for c in cue[i]:\n","                                    cue_cues[-1][c] = 2\n","                                    scope_cues[-1][c] = 2\n","                            scope_scopes.append([0]*len(sentence))\n","                            if i in scope.keys():\n","                                for s in scope[i]:\n","                                    scope_scopes[-1][s] = 1\n","                    sentence = []\n","                    cue = {}\n","                    scope = {}\n","                    in_scope = []\n","                    in_cue = []\n","                    word_num = 0\n","                    in_word = 0\n","                    c_idx = []\n","                    s_idx = []\n","                elif '<' not in token:\n","                    if in_word == 1:\n","                        if len(in_cue)!=0:\n","                            for i in in_cue:\n","                                cue[i].append(word_num)\n","                        if len(in_scope)!=0:\n","                            for i in in_scope:\n","                                for j in i:\n","                                    scope[j].append(word_num)\n","                        sentence.append(token)\n","            cue_only_samples = random.sample(cue_only_data, k=int(frac_no_cue_sents*len(cue_only_data)))\n","            cue_only_sents = [i[0] for i in cue_only_samples]\n","            cue_only_cues = [i[1] for i in cue_only_samples]\n","            return [(cue_sentence+cue_only_sents, cue_cues+cue_only_cues),(scope_sentence, scope_cues, scope_scopes)]\n","        \n","\n","        if dataset_name == 'bioscope':\n","            ret_val = bioscope(file, frac_no_cue_sents=frac_no_cue_sents)\n","            scope_sents, scope_cues, data_scope = ret_val[1]\n","            for item in zip(scope_sents, scope_cues, data_scope):\n","              self.the_big_data_list.append(item)\n","\n","        elif dataset_name == 'sfu':\n","            sfu_cues = [[], []]\n","            sfu_scopes = [[], [], []]\n","            for dir_name in os.listdir(file):\n","                if '.' not in dir_name:\n","                    for f_name in os.listdir(file+\"//\"+dir_name):\n","                        r_val = sfu_review(file+\"//\"+dir_name+'//'+f_name, frac_no_cue_sents=frac_no_cue_sents)\n","                        sfu_cues = [a+b for a,b in zip(sfu_cues, r_val[0])]\n","                        sfu_scopes = [a+b for a,b in zip(sfu_scopes, r_val[1])]\n","            scope_sents, scope_cues, data_scope = sfu_scopes\n","            for item in zip(scope_sents, scope_cues, data_scope):\n","              self.the_big_data_list.append(item)\n","        elif dataset_name == 'starsem':\n","            if TASK == 'negation':\n","                ret_val = starsem(file, frac_no_cue_sents=frac_no_cue_sents)\n","                scope_sents, scope_cues, data_scope = ret_val[1]\n","                for item in zip(scope_sents, scope_cues, data_scope):\n","                  self.the_big_data_list.append(item)\n","\n","        else:\n","            raise ValueError(\"Supported Dataset types are:\\n\\tbioscope\\n\\tsfu\\n\\tconll_cue\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"26maAzM7aENL"},"source":["bioscope_full_papers_data = Data('/content/gdrive/My Drive/multilingual_BERT_negations/data_raw/bioscope/full_papers.xml', dataset_name='bioscope')\n","sfu_data = Data('/content/gdrive/My Drive/multilingual_BERT_negations/data_raw/SFU_Review_Corpus_Negation_Speculation', dataset_name='sfu')\n","bioscope_abstracts_data = Data('/content/gdrive/My Drive/multilingual_BERT_negations/data_raw/bioscope/abstracts.xml', dataset_name='bioscope')\n","if TASK == 'negation':\n","    sherlock_train_data = Data('/content/gdrive/My Drive/multilingual_BERT_negations/data_raw/starsem/SEM-2012-SharedTask-CD-SCO-training-09032012.txt', dataset_name='starsem')\n","    sherlock_dev_data = Data('/content/gdrive/My Drive/multilingual_BERT_negations/data_raw/starsem/SEM-2012-SharedTask-CD-SCO-dev-09032012.txt', dataset_name='starsem')\n","    sherlock_test_gold_cardboard_data = Data('/content/gdrive/My Drive/multilingual_BERT_negations/data_raw/starsem/SEM-2012-SharedTask-CD-SCO-test-cardboard-GOLD.txt', dataset_name='starsem')\n","    sherlock_test_gold_circle_data = Data('/content/gdrive/My Drive/multilingual_BERT_negations/data_raw/starsem/SEM-2012-SharedTask-CD-SCO-test-circle-GOLD.txt', dataset_name='starsem')\n","    french_other = Data('/content/gdrive/My Drive/multilingual_BERT_negations/data_raw/CAS_sherlock_one_scope.txt', dataset_name='starsem')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E81oghpzOXS-"},"source":["all_data = bioscope_full_papers_data.the_big_data_list + bioscope_abstracts_data.the_big_data_list + sfu_data.the_big_data_list + sherlock_train_data.the_big_data_list + sherlock_dev_data.the_big_data_list + sherlock_test_gold_cardboard_data.the_big_data_list + sherlock_test_gold_circle_data.the_big_data_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7ES9PpQgZiF"},"source":["Bioscope =  bioscope_full_papers_data.the_big_data_list + bioscope_abstracts_data.the_big_data_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ItNSnmYckP3x"},"source":["Sherlock = sherlock_train_data.the_big_data_list + sherlock_dev_data.the_big_data_list + sherlock_test_gold_cardboard_data.the_big_data_list + sherlock_test_gold_circle_data.the_big_data_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UHYA-aRfkW44"},"source":["SFU = sfu_data.the_big_data_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SKCHgV_NUXk_","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594214205746,"user_tz":-120,"elapsed":524,"user":{"displayName":"Anastassia Shaitarova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64","userId":"01489516284222648289"}},"outputId":"aab28c3d-f112-40db-a498-6c337ba04c3c"},"source":["len(bioscope_full_papers_data.the_big_data_list)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["376"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"eUkA00unW6fh","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594214207494,"user_tz":-120,"elapsed":731,"user":{"displayName":"Anastassia Shaitarova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64","userId":"01489516284222648289"}},"outputId":"91ca43cc-fb2c-4b1d-a06d-9b77353c4271"},"source":["len(bioscope_abstracts_data.the_big_data_list)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1719"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"6pjZiv7akbUt","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594214214409,"user_tz":-120,"elapsed":746,"user":{"displayName":"Anastassia Shaitarova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64","userId":"01489516284222648289"}},"outputId":"2830d5af-f467-4c7d-affe-3c1d32d42e04"},"source":["len(Bioscope)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2095"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"VwN8JppKXq6B","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594214217986,"user_tz":-120,"elapsed":524,"user":{"displayName":"Anastassia Shaitarova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64","userId":"01489516284222648289"}},"outputId":"2fb60881-281b-4395-b0cc-0ac79ce4447d"},"source":["len(SFU)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3528"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"kdBZ7rXNXwQh","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594214220595,"user_tz":-120,"elapsed":537,"user":{"displayName":"Anastassia Shaitarova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64","userId":"01489516284222648289"}},"outputId":"234933de-5189-46e4-aaab-d023ec40cf39"},"source":["len(Sherlock)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1421"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"j7qugEhSGqo7","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594214223296,"user_tz":-120,"elapsed":559,"user":{"displayName":"Anastassia Shaitarova","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64","userId":"01489516284222648289"}},"outputId":"d549e96e-2c21-4aaf-f26b-db543dee5ed2"},"source":["len(all_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7044"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"oqfXjTXcHv3F"},"source":["outfile = format('/content/gdrive/My Drive/multilingual_BERT_negations/data/all_ENG_sent_cue_scope.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sGI7Y8bvHqCB"},"source":["count_sents = 0\n","hash_values = set()\n","with open(outfile, 'w', encoding='utf8') as outf:\n","  for sentence in all_data:\n","    scope_tokens = []\n","    sent = ' '.join(sentence[0])\n","    scope = sentence[2]\n","    cue = ''\n","    for item in zip(sentence[0], scope, sentence[1]):\n","      if item[1] == 1:\n","        scope_tokens.append(item[0])\n","      if item[2] != 3:\n","        cue += ' '+item[0]\n","\n","    count_sents += 1\n","    outf.write(str(count_sents)+'\\n')\n","    outf.write(sent+'\\n')\n","    outf.write(str(sentence[1])+'\\n')\n","    outf.write(str(scope)+'\\n')\n","    outf.write(cue.strip()+'\\n')\n","    outf.write(' '.join(scope_tokens)+'\\n')\n","    outf.write('\\n')\n","\n","count_sents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N_gyDMW4TjwS"},"source":["for item in sfu_data.the_big_data_list:\n","  print(item[0])\n","  print(item[1])\n","  print(item[2])\n","  print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f9_CzK6aBbWD"},"source":["# Choose file to write to."]},{"cell_type":"code","metadata":{"id":"xfO_sVRMS5Cv"},"source":["outfile = format('/content/gdrive/My Drive/multilingual_BERT_negations/data/ENG.json')\n","with open(outfile, 'w') as outf:\n","  json.dump(all_data, outf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IsMyGjwlS1MD"},"source":["outfile = format('/content/gdrive/My Drive/multilingual_BERT_negations/data/SHERLOCK.json')\n","with open(outfile, 'w') as outf:\n","  json.dump(ENG_FR, outf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D2tFztidf26F"},"source":["outfile = format('/content/gdrive/My Drive/multilingual_BERT_negations/data/SFU.json')\n","with open(outfile, 'w') as outf:\n","  json.dump(sfu_data.the_big_data_list, outf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhcfG3qggTIm"},"source":["outfile = format('/content/gdrive/My Drive/multilingual_BERT_negations/data/BIOSCOPE.json')\n","with open(outfile, 'w') as outf:\n","  json.dump(Bioscope, outf)"],"execution_count":null,"outputs":[]}]}