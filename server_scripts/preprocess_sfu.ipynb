{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGTg8YDGGbCy"
   },
   "outputs": [],
   "source": [
    "# Imports needed\n",
    "import os, re, torch, html, tempfile, copy, json, math, shutil, tarfile, tempfile, sys, random, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tq0WavgDIGJy"
   },
   "source": [
    "Upload the datasets to Google Drive. \n",
    "This allows access to your Google Drive from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22822,
     "status": "ok",
     "timestamp": 1587715095958,
     "user": {
      "displayName": "Anastassia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64",
      "userId": "01489516284222648289"
     },
     "user_tz": -120
    },
    "id": "h7_IpJcCHOGK",
    "outputId": "59439c2c-0d4d-4fbb-a039-28cf2a02e583"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0BuBr5yWJ1Cj"
   },
   "source": [
    "# Class Definitions:\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQQnT6Kydye8"
   },
   "outputs": [],
   "source": [
    "sfu_data = '/content/gdrive/My Drive/multilingual_BERT_negations/data/SFU_Review_Corpus_Negation_Speculation'\n",
    "frac_no_cue_sents = 1.0\n",
    "TASK = 'negation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe1kG3IZKBrD"
   },
   "outputs": [],
   "source": [
    "def sfu_review(f_path, cue_sents_only=False, frac_no_cue_sents = 1.0):\n",
    "    # print(f_path)\n",
    "    file = open(f_path, encoding = 'utf-8')\n",
    "    sentences = []\n",
    "    for s in file:\n",
    "        print(s)\n",
    "        sentences+=re.split(\"(<.*?>)\", html.unescape(s))\n",
    "    cue_sentence = []\n",
    "    cue_cues = []\n",
    "    scope_cues = []\n",
    "    scope_scopes = []\n",
    "    scope_sentence = []\n",
    "    sentence = []\n",
    "    cue = {}\n",
    "    scope = {}\n",
    "    in_scope = []\n",
    "    in_cue = []\n",
    "    word_num = 0\n",
    "    c_idx = []\n",
    "    cue_only_data = []\n",
    "    s_idx = []\n",
    "    in_word = 0\n",
    "    print(len(sentences))\n",
    "    print(sentences)\n",
    "    for token in sentences:\n",
    "        if token == '':\n",
    "            continue\n",
    "        elif token == '<W>':\n",
    "            in_word = 1\n",
    "        elif token == '</W>':\n",
    "            in_word = 0\n",
    "            word_num += 1\n",
    "        elif '<cue' in token:\n",
    "            if TASK in token:\n",
    "                in_cue.append(int(re.split('(ID=\".*?\")',token)[1][4:-1]))\n",
    "                c_idx.append(int(re.split('(ID=\".*?\")',token)[1][4:-1]))\n",
    "                cue[c_idx[-1]] = []\n",
    "        elif '</cue' in token:\n",
    "            in_cue = in_cue[:-1]\n",
    "        elif '<xcope' in token:\n",
    "            continue\n",
    "        elif '</xcope' in token:\n",
    "            in_scope = in_scope[:-1]\n",
    "        elif '<ref' in token:\n",
    "            in_scope.append([int(i) for i in re.split('(SRC=\".*?\")',token)[1][5:-1].split(' ')])\n",
    "            s_idx.append([int(i) for i in re.split('(SRC=\".*?\")',token)[1][5:-1].split(' ')])\n",
    "            for i in s_idx[-1]:\n",
    "                scope[i] = []\n",
    "        elif '</SENTENCE' in token:\n",
    "            if len(cue.keys())==0:\n",
    "                cue_only_data.append([sentence, [3]*len(sentence)])\n",
    "            else:\n",
    "                cue_sentence.append(sentence)\n",
    "                cue_cues.append([3]*len(sentence))\n",
    "                for i in cue.keys():\n",
    "                    scope_sentence.append(sentence)\n",
    "                    scope_cues.append([3]*len(sentence))\n",
    "                    if len(cue[i])==1:\n",
    "                        cue_cues[-1][cue[i][0]] = 1\n",
    "                        scope_cues[-1][cue[i][0]] = 1\n",
    "                    else:\n",
    "                        for c in cue[i]:\n",
    "                            cue_cues[-1][c] = 2\n",
    "                            scope_cues[-1][c] = 2\n",
    "                    scope_scopes.append([0]*len(sentence))\n",
    "                    if i in scope.keys():\n",
    "                        for s in scope[i]:\n",
    "                            scope_scopes[-1][s] = 1\n",
    "            sentence = []\n",
    "            cue = {}\n",
    "            scope = {}\n",
    "            in_scope = []\n",
    "            in_cue = []\n",
    "            word_num = 0\n",
    "            in_word = 0\n",
    "            c_idx = []\n",
    "            s_idx = []\n",
    "        elif '<' not in token:\n",
    "            if in_word == 1:\n",
    "                if len(in_cue)!=0:\n",
    "                    for i in in_cue:\n",
    "                        cue[i].append(word_num)\n",
    "                if len(in_scope)!=0:\n",
    "                    for i in in_scope:\n",
    "                        for j in i:\n",
    "                            scope[j].append(word_num)\n",
    "                sentence.append(token)\n",
    "    cue_only_samples = random.sample(cue_only_data, k=int(frac_no_cue_sents*len(cue_only_data)))\n",
    "    cue_only_sents = [i[0] for i in cue_only_samples]\n",
    "    cue_only_cues = [i[1] for i in cue_only_samples]\n",
    "    return [(cue_sentence+cue_only_sents, cue_cues+cue_only_cues),(scope_sentence, scope_cues, scope_scopes)]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7flcwFZKSF4L"
   },
   "outputs": [],
   "source": [
    "sfu_cues = [[], []]\n",
    "sfu_scopes = [[], [], []]\n",
    "for dir_name in os.listdir(sfu_data):\n",
    "    if '.' not in dir_name:\n",
    "        for f_name in os.listdir(sfu_data+\"//\"+dir_name):\n",
    "            r_val = sfu_review(sfu_data+\"//\"+dir_name+'//'+f_name, frac_no_cue_sents=frac_no_cue_sents)\n",
    "            sfu_cues = [a+b for a,b in zip(sfu_cues, r_val[0])]\n",
    "            sfu_scopes = [a+b for a,b in zip(sfu_scopes, r_val[1])]\n",
    "\n",
    "scope_sents, scope_cues, data_scope = sfu_scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jiyGM-MmvElH"
   },
   "outputs": [],
   "source": [
    "for item in zip(scope_sents, scope_cues, data_scope):\n",
    "  print(item[0])\n",
    "  print(item[1])\n",
    "  print(item[2])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hr9AXN9fMI2R"
   },
   "outputs": [],
   "source": [
    "method = SCOPE_METHOD\n",
    "do_lower_case = True\n",
    "tokenizer = BertTokenizer.from_pretrained(SCOPE_MODEL, do_lower_case=do_lower_case, cache_dir='bert_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEQ1qFrgdQb2"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(scope_sents, scope_cues, data_scope):\n",
    "  dl_sents = scope_sents\n",
    "  dl_cues = scope_cues\n",
    "  dl_scopes = data_scope\n",
    "    \n",
    "  sentences = [\" \".join([s for s in sent]) for sent in dl_sents]\n",
    "  mytexts = []\n",
    "  mylabels = []\n",
    "  mycues = []\n",
    "  mymasks = []\n",
    "  if do_lower_case == True:\n",
    "      sentences_clean = [sent.lower() for sent in sentences]\n",
    "  else:\n",
    "      sentences_clean = sentences\n",
    "  \n",
    "  for sent, tags, cues in zip(sentences_clean,dl_scopes, dl_cues):\n",
    "      new_tags = []\n",
    "      new_text = []\n",
    "      new_cues = []\n",
    "      new_masks = []\n",
    "      for word, tag, cue in zip(sent.split(),tags,cues):\n",
    "          sub_words = tokenizer._tokenize(word)\n",
    "          for count, sub_word in enumerate(sub_words):\n",
    "              mask = 1\n",
    "              if count > 0:\n",
    "                  mask = 0\n",
    "              new_masks.append(mask)\n",
    "              new_tags.append(tag)\n",
    "              new_cues.append(cue)\n",
    "              new_text.append(sub_word)\n",
    "      mymasks.append(new_masks)\n",
    "      mytexts.append(new_text)\n",
    "      mylabels.append(new_tags)\n",
    "      mycues.append(new_cues)\n",
    "\n",
    "  final_sentences = []\n",
    "  final_labels = []\n",
    "  final_masks = []\n",
    "\n",
    "  for sent,cues,labels,masks in zip(mytexts, mycues, mylabels, mymasks):\n",
    "    temp_sent = []\n",
    "    temp_label = []\n",
    "    temp_masks = []\n",
    "    first_part = 0\n",
    "    for token,cue,label,mask in zip(sent,cues,labels,masks):\n",
    "        if cue!=3:\n",
    "            if first_part == 0:\n",
    "                first_part = 1\n",
    "                temp_sent.append(f'[unused{cue+1}]')\n",
    "                temp_masks.append(1)\n",
    "                temp_label.append(0)\n",
    "                temp_sent.append(token)\n",
    "                temp_masks.append(0)\n",
    "                temp_label.append(label)\n",
    "                continue\n",
    "            temp_sent.append(f'[unused{cue+1}]')\n",
    "            temp_masks.append(0)\n",
    "            temp_label.append(0)\n",
    "        else:\n",
    "            first_part = 0\n",
    "        temp_masks.append(mask)\n",
    "        temp_sent.append(token)\n",
    "        temp_label.append(label)\n",
    "    temp_sent.insert(0, '[CLS]')\n",
    "    temp_sent.append('[SEP]')\n",
    "    temp_masks.append(0)\n",
    "    temp_label.append(0)\n",
    "    temp_label.insert(0, 0)\n",
    "    temp_masks.insert(0, 0)\n",
    "    final_sentences.append(temp_sent)\n",
    "    final_labels.append(temp_label)\n",
    "    final_masks.append(temp_masks)\n",
    "\n",
    "\n",
    "  return final_sentences, final_labels, final_masks         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YBuv0AgMeUfQ"
   },
   "outputs": [],
   "source": [
    "final_sentences, final_labels, final_masks = preprocess_data(scope_sents, scope_cues, data_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Co5Y0rngCuv"
   },
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([[tokenizer._convert_token_to_id(word) for word in txt] for txt in final_sentences],\n",
    "                                      maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Za-sqdZhf72d"
   },
   "outputs": [],
   "source": [
    "tags = pad_sequences(final_labels, maxlen=MAX_LEN, value=0, padding=\"post\", dtype=\"long\", truncating=\"post\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e1RZ0loygk3Q"
   },
   "outputs": [],
   "source": [
    "finalest_masks = pad_sequences(final_masks, maxlen=MAX_LEN, value=0, padding='post', dtype='long', truncating='post').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgajZJAhgmfM"
   },
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1105,
     "status": "ok",
     "timestamp": 1587543542396,
     "user": {
      "displayName": "Anastassia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64",
      "userId": "01489516284222648289"
     },
     "user_tz": -120
    },
    "id": "I0whM2jJlL23",
    "outputId": "ac47dd0e-a556-4a1a-eaea-ddc402382d40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There\\x92s', 'also', 'plenty', 'of', 'historical', 'information', ',', 'which', 'is', 'integrated', 'smoothly', 'into', 'the', 'prose', ';', 'remarkably', ',', 'even', 'when', 'inserted', 'into', 'dialogue', ',', 'the', 'explanatory', 'asides', 'don\\x92t', 'interrupt', 'the', 'flow', '.']\n",
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3]\n",
      "True scope labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(scope_sents[3])\n",
    "print(scope_cues[3])\n",
    "print('True scope labels:', data_scope[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LV25xN7P-OfI"
   },
   "outputs": [],
   "source": [
    "outfile = format('/content/gdrive/My Drive/multilingual_BERT_negations/data/sfu_sentences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1067,
     "status": "ok",
     "timestamp": 1587545316137,
     "user": {
      "displayName": "Anastassia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64",
      "userId": "01489516284222648289"
     },
     "user_tz": -120
    },
    "id": "foLwPUTO-cyh",
    "outputId": "24a59ea9-8102-4dba-b57c-75096d2ac6b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no flowery dialog , and time is n't wasted trying to establish a hot relationship between the two lead characters .\n",
      "When you first open the book , reading the Prologue , it grips you , right then , right there , so early on , you have no choice but to read , quickly , with anticipation , knowing in your heart that Alex has to be triumphant ! , does n't he ?\n",
      "Additionally , the G5 no longer supports the Apple Pro Speaker connector , so I ca n't use the Griffin ProSpeaker Cable to connect speakers to the machine .\n",
      "For now , I 've banned my kids from even coming within 5 feet of this baby , though I know I wo n't be able to enforce the \" no kid zone \" for long .\n",
      "A long time ago I returned a new desktop PC to Gateway because I could n't operate it intuitively ( my fault ) , and Gateway graciously took it back , no questions asked .\n",
      "My iMac was trouble from the start and I got no help from Apple other than advice , and after 90 days I could n't even get that unless I forked over an extra [ $ $ $ ] for an extended warranty , which I did .\n",
      "I have no idea why Apple choose not to include a 2-button mouse on this machine , as the OS X operating system supposedly does support 2-button mouse operation ( although it supposedly does n't offer as extensive support for 2-button operation as compared to Windows ) .\n",
      "\" cuz the entree ai n't as good without somethin on the side \" is one of those lines that i 'm just gonna remember no matter what .\n",
      "There 's no point in singing this - it tells basically the same type of story the rest of the songs do - except , story is n't the word to describe it .\n",
      "Cash Money Millionaires came along many years ago , but were n't recognized until 1998 when they had a huge empire of hit-making south rappers with no talent , but catchy songs .\n",
      "Make no mistake , this ai n't a Lexus , or even a Camry in terms of being quiet .\n",
      "While the Mustang is n't among that list of cars , it certainly is no slouch providing MacPherson struts in the front and a variable rate , Quadra shock rear suspension with Traction Lok rear axle and stabilizer bar .\n",
      "Three Lexus models ( RX300-did n't want SUV, IS-300 , cool car but no leather option and did n't want RWD , and the GS300 , which is a really nice car , but glove compartment-mounted CD player ruled it out )\n",
      "They let men treat them brutishly ( no physical abuse , mind you--the women just are n't allowed to think for themselves ) .\n",
      "Acting wise , Cruise stays on top of his game , no I do n't think it is \" Oscar-worthy \" despite the amount of training he did for the role , but he really has n't been better .\n",
      "And she ai n't no Rossellini .\n",
      "Because there are no attractions for children , you wo n't find them wandering through the casino or hotel hallways .\n",
      "This machine has just a tiny red light , so if you call-screen a message and do n't erase it , you have no idea whether other messages have been added (unless you start pushing buttons ) .\n",
      "The only thing that I do n't like about this product is that each handset is an island unto itself ; there is no way to synchronize the address books between handsets .\n",
      "This feature is called forward no answer like on company phones if you do n't pick up after 2 rings it will go right into your voicemail and your personal greeting will play .\n",
      "Well if peeling and scratching are n't covered under this \"WARRANTY \" after less than 3 months use then I have no clue what is covered .\n",
      "Chef Boyardee and I became chummy for many months , and his concoction ( no , I do n't want to know what 's in it ) simmered to preservative-laden perfection within my All-Clad Stainless 1-Quart Sauce Pan .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3528"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_sents = 0\n",
    "hash_values = set()\n",
    "with open(outfile, 'w', encoding='utf8') as outf:\n",
    "  for sent in scope_sents:\n",
    "    sent = ' '.join(sent)\n",
    "    sent_hash = hash(sent)\n",
    "    count_sents += 1\n",
    "    if sent_hash not in hash_values:\n",
    "      hash_values.add(sent_hash)\n",
    "      if all(token in sent.split() for token in [\"n't\", 'no']):\n",
    "        print(sent)\n",
    "    # outf.write(' '.join(sent))\n",
    "\n",
    "count_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWOeW1yJ0fCE"
   },
   "source": [
    "## INPUT for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 582,
     "status": "ok",
     "timestamp": 1579599761496,
     "user": {
      "displayName": "Anastassia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCM73UvegjeDg8-8l0xRslSs-OFHTUslJXHHsOO5Q=s64",
      "userId": "01489516284222648289"
     },
     "user_tz": -60
    },
    "id": "q6K420yQ0cF_",
    "outputId": "4861d302-a685-4767-d18c-ae4aaba255b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'but', 'the', 'morning', 'paper', 'was', '[unused1]', 'un', '[unused1]', '##int', '[unused1]', '##eres', '[unused1]', '##ting', '.', '[SEP]']\n",
      "16\n",
      "\n",
      "input_ids:  [101, 2021, 1996, 2851, 3259, 2001, 2, 4895, 2, 18447, 2, 18702, 2, 3436, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "scope ids:  [0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "true token masks:  [0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention masks:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "x = 3\n",
    "print(final_sentences[x])\n",
    "print(len(final_sentences[x]))\n",
    "print()\n",
    "print(\"input_ids: \", input_ids[x])\n",
    "print(\"scope ids: \", tags[x])\n",
    "print(\"true token masks: \", finalest_masks[x])\n",
    "print('attention masks: ', attention_masks[x])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocess_sfu.ipynb",
   "provenance": [
    {
     "file_id": "1nKZVSzdle4nc34g-XE4WmJ6KCTEC50ZA",
     "timestamp": 1579617709114
    },
    {
     "file_id": "1EMGz9atX_0rV906ynWj1lHGXhXuM9MvW",
     "timestamp": 1579508607571
    },
    {
     "file_id": "1jmoUcOE7lOwp-fqj6wjtHZpe92ob3Xom",
     "timestamp": 1579171866963
    },
    {
     "file_id": "https://github.com/adityak6798/Transformers-For-Negation-and-Speculation/blob/master/Transformers_for_Negation_and_Speculation.ipynb",
     "timestamp": 1578674765715
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
