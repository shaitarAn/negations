{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcsQGMrRGJsH"
   },
   "outputs": [],
   "source": [
    "# Installations needed\n",
    "!pip install transformers\n",
    "!pip install knockknock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PenMagTOWnWw"
   },
   "outputs": [],
   "source": [
    "!pip install -U ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGTg8YDGGbCy"
   },
   "outputs": [],
   "source": [
    "# Imports needed\n",
    "import os, re, torch, html, tempfile, copy, json, math, shutil, tarfile, tempfile, sys, random, pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertTokenizer, BertConfig, BertModel, WordpieceTokenizer\n",
    "from transformers.file_utils import cached_path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tq0WavgDIGJy"
   },
   "source": [
    "Upload the datasets to Google Drive. \n",
    "This allows access to your Google Drive from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13238,
     "status": "ok",
     "timestamp": 1583333724410,
     "user": {
      "displayName": "Anastassia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64",
      "userId": "01489516284222648289"
     },
     "user_tz": -60
    },
    "id": "h7_IpJcCHOGK",
    "outputId": "686dfb3b-8db8-4955-82e9-c46101eaf236"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0BuBr5yWJ1Cj"
   },
   "source": [
    "# Class Definitions:\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQQnT6Kydye8"
   },
   "outputs": [],
   "source": [
    "sherlock_data = '/content/gdrive/My Drive/multilingual_BERT_negations/data/CAS_sherlock_full.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe1kG3IZKBrD"
   },
   "outputs": [],
   "source": [
    "def starsem(f_path, cue_sents_only=False, frac_no_cue_sents = 1.0):\n",
    "    raw_data = open(f_path)\n",
    "    sentence = []\n",
    "    labels = []\n",
    "    label = []\n",
    "    scope_sents = []\n",
    "    data_scope = []\n",
    "    scope = []\n",
    "    scope_cues = []\n",
    "    # list of lists of all sentences\n",
    "    data = []\n",
    "    cue_only_data = []\n",
    "    \n",
    "    for line in raw_data:\n",
    "        # print(line)\n",
    "        label = []\n",
    "        sentence = []\n",
    "        tokens = line.strip().split()\n",
    "        if len(tokens)==8: #This line has no cues\n",
    "                print(tokens)\n",
    "                # append the word\n",
    "                sentence.append(tokens[3])\n",
    "                label.append(3) #Not a cue\n",
    "                for line in raw_data:\n",
    "                    tokens = line.strip().split()\n",
    "                    if len(tokens)==0:\n",
    "                        break\n",
    "                    else:\n",
    "                        sentence.append(tokens[3])\n",
    "                        label.append(3)\n",
    "                cue_only_data.append([sentence, label])\n",
    "                \n",
    "        # elif len(tokens) == 1:\n",
    "          # print(tokens)  \n",
    "        else: #The line has 1 or more cues\n",
    "            num_cues = (len(tokens)-7)//3\n",
    "            #cue_count+=num_cues\n",
    "            scope = [[] for i in range(num_cues)]\n",
    "            label = [[],[]] #First list is the real labels, second list is to modify if it is a multi-word cue.\n",
    "            label[0].append(3) #Generally not a cue, if it is will be set ahead.\n",
    "            label[1].append(-1) #Since not a cue, for now.\n",
    "            # print(label)\n",
    "            for i in range(num_cues):\n",
    "                if tokens[7+3*i] != '_': #Cue field is active\n",
    "                    if tokens[8+3*i] != '_': #Check for affix\n",
    "                        label[0][-1] = 0 #Affix\n",
    "                        # affix_list.append(tokens[7+3*i])\n",
    "                        label[1][-1] = i #Cue number\n",
    "                        #sentence.append(tokens[7+3*i])\n",
    "                        #new_word = '##'+tokens[8+3*i]\n",
    "                    else:\n",
    "                        label[0][-1] = 1 #Maybe a normal or multiword cue. The next few words will determine which.\n",
    "                        label[1][-1] = i #Which cue field, for multiword cue altering.\n",
    "                        \n",
    "                if tokens[8+3*i] != '_':\n",
    "                    scope[i].append(1)\n",
    "                else:\n",
    "                    scope[i].append(0)\n",
    "            sentence.append(tokens[3])\n",
    "            for line in raw_data:\n",
    "                tokens = line.strip().split()\n",
    "                if len(tokens)==0:\n",
    "                    break\n",
    "                else:\n",
    "                    sentence.append(tokens[3])\n",
    "                    label[0].append(3) #Generally not a cue, if it is will be set ahead.\n",
    "                    label[1].append(-1) #Since not a cue, for now.   \n",
    "                    for i in range(num_cues):\n",
    "                        if tokens[7+3*i] != '_': #Cue field is active\n",
    "                            if tokens[8+3*i] != '_': #Check for affix\n",
    "                                label[0][-1] = 0 #Affix\n",
    "                                label[1][-1] = i #Cue number\n",
    "                            else:\n",
    "                                label[0][-1] = 1 #Maybe a normal or multiword cue. The next few words will determine which.\n",
    "                                label[1][-1] = i #Which cue field, for multiword cue altering.\n",
    "                        if tokens[8+3*i] != '_':\n",
    "                            scope[i].append(1)\n",
    "                        else:\n",
    "                            scope[i].append(0)\n",
    "            for i in range(num_cues):\n",
    "                indices = [index for index,j in enumerate(label[1]) if i==j]\n",
    "                count = len(indices)\n",
    "                if count>1:\n",
    "                    for j in indices:\n",
    "                        label[0][j] = 2\n",
    "            for i in range(num_cues):\n",
    "                sc = []\n",
    "                for a,b in zip(label[0],label[1]):\n",
    "                    if i==b:\n",
    "                        sc.append(a)\n",
    "                    else:\n",
    "                        sc.append(3)\n",
    "                scope_cues.append(sc)\n",
    "                scope_sents.append(sentence)\n",
    "                data_scope.append(scope[i])\n",
    "            labels.append(label[0])\n",
    "            # print(labels)\n",
    "            data.append(sentence)\n",
    "    # print(type(data))\n",
    "    # print(data)\n",
    "    cue_only_samples = random.sample(cue_only_data, k=int(frac_no_cue_sents*len(cue_only_data)))\n",
    "    cue_only_sents = [i[0] for i in cue_only_samples]\n",
    "    cue_only_cues = [i[1] for i in cue_only_samples]\n",
    "    starsem_cues = (data+cue_only_sents,labels+cue_only_cues)\n",
    "    starsem_scopes = (scope_sents, scope_cues, data_scope)\n",
    "    return [starsem_cues, starsem_scopes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7flcwFZKSF4L"
   },
   "outputs": [],
   "source": [
    "starsem_cues, starsem_scopes = starsem(sherlock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JbDnVo7RddsM"
   },
   "outputs": [],
   "source": [
    "scope_sents, scope_cues, data_scope = starsem_scopes\n",
    "scope_data = zip(scope_sents, scope_cues, data_scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 632,
     "status": "ok",
     "timestamp": 1583337746292,
     "user": {
      "displayName": "Anastassia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64",
      "userId": "01489516284222648289"
     },
     "user_tz": -60
    },
    "id": "I0whM2jJlL23",
    "outputId": "7eea4c90-7122-47d4-eda8-84528857edec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"l'\", 'examen', 'endoscopique', 'bronchique', 'ne', 'révèle', 'aucune', 'anomalie', '.']\n",
      "[3, 3, 3, 3, 2, 3, 2, 3, 3]\n",
      "Scope labels: [0, 0, 0, 0, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(scope_sents[3])\n",
    "print(scope_cues[3])\n",
    "print('Scope labels:', data_scope[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VxWHdxyaNs79"
   },
   "source": [
    "Tokenize with BERT and convert the input to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0--9vurTqgIS"
   },
   "outputs": [],
   "source": [
    "SCOPE_MODEL = 'bert-base-uncased'\n",
    "# if 'bert-base-multilingual-cased' do_lower_case=False\n",
    "do_lower_case = True\n",
    "MAX_LEN = 128\n",
    "method = 'augment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlN2xBhOIXwB"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "7a80e0f025b649258b3cd555a2e256f2",
      "741977bbaaf349009cf63766f7d935c7",
      "e83950142d5543c3ac2d8143c1007a45",
      "bdd92ea2c9fb4699b819ba23ee5d06a0",
      "3fd5aa1c48954f30ad7d025ea449d930",
      "8a4cfe1b71404c5387eba53cd6cdab95",
      "a7fe35fb1f414f849de09bc45a272697",
      "7e597852111c4ad8ba91f6e1e969a645"
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1583337423431,
     "user": {
      "displayName": "Anastassia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64",
      "userId": "01489516284222648289"
     },
     "user_tz": -60
    },
    "id": "Hr9AXN9fMI2R",
    "outputId": "0a015040-fca5-4e41-f8b9-78952871e14a"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(SCOPE_MODEL, do_lower_case=do_lower_case, cache_dir='bert_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zjGe5nobpnSY"
   },
   "outputs": [],
   "source": [
    "# ####################################\n",
    "# collect example of BERT tokenization\n",
    "sents = [\" \".join([s for s in sent]) for sent in scope_sents]\n",
    "[sent.lower() for sent in sents]\n",
    "\n",
    "for word in sents[1].split():\n",
    "  print(word)\n",
    "  subwords = tokenizer.tokenize(word)\n",
    "  print(word, subwords)\n",
    "\n",
    "# au\n",
    "# au ['au']\n",
    "# niveau\n",
    "# niveau ['niveau']\n",
    "# thoracique\n",
    "# thoracique ['th', '##ora', '##ci', '##que']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEQ1qFrgdQb2"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(zipped, tokenizer_obj):\n",
    "    \n",
    "    zipped = list(zipped)\n",
    "    dl_sents = [item[0] for item in zipped]\n",
    "    dl_cues = [item[1] for item in zipped]\n",
    "    dl_scopes = [item[2] for item in zipped]\n",
    "    \n",
    "    # print(dl_sents[1])\n",
    "    sentences = [\" \".join([s for s in sent]) for sent in dl_sents]\n",
    "    mytexts = []\n",
    "    mylabels = []\n",
    "    mycues = []\n",
    "    mymasks = []\n",
    "    if do_lower_case == True:\n",
    "        sentences_clean = [sent.lower() for sent in sentences]\n",
    "    else:\n",
    "        sentences_clean = sentences\n",
    "    \n",
    "    for sent, tags, cues in zip(sentences_clean, dl_scopes, dl_cues):\n",
    "        new_tags = []\n",
    "        new_text = []\n",
    "        new_cues = []\n",
    "        new_masks = []\n",
    "        for word, tag, cue in zip(sent.split(), tags, cues):\n",
    "            sub_words = tokenizer_obj._tokenize(word)\n",
    "\n",
    "            # update tags and labels for subwords\n",
    "            for count, sub_word in enumerate(sub_words):\n",
    "                # the first subword is the true token\n",
    "                mask = 1\n",
    "                if count > 0:\n",
    "                    # all the other subwords that start with # receive a mask 0\n",
    "                    # these will be the true_token_masks\n",
    "                    mask = 0\n",
    "\n",
    "                # start collecting masks for true and not true tokens\n",
    "                new_masks.append(mask)\n",
    "\n",
    "                # tags(scope) and cue labels stay the same for all the subwords\n",
    "                new_tags.append(tag)\n",
    "                new_cues.append(cue)\n",
    "                new_text.append(sub_word)\n",
    "        # true and not true tokens\n",
    "        mymasks.append(new_masks)\n",
    "        mytexts.append(new_text)\n",
    "        mylabels.append(new_tags)\n",
    "        mycues.append(new_cues)\n",
    "\n",
    "    # start lists of final inputs\n",
    "    final_sentences = []\n",
    "    final_labels = []\n",
    "    final_masks = []\n",
    "    if method == 'replace':\n",
    "        for sent,cues in zip(mytexts, mycues):\n",
    "            temp_sent = []\n",
    "            for token,cue in zip(sent,cues):\n",
    "                if cue==3:\n",
    "                    temp_sent.append(token)\n",
    "                else:\n",
    "                    temp_sent.append(f'[unused{cue+1}]')\n",
    "            final_sentences.append(temp_sent)\n",
    "        final_labels = mylabels\n",
    "        final_masks = mymasks\n",
    "    elif method == 'augment':\n",
    "        # mylabels are scope tags\n",
    "        # input is already subword-tokenized by BERT\n",
    "        for sent, cues, labels, masks in zip(mytexts, mycues, mylabels, mymasks):\n",
    "          \n",
    "            temp_sent = []\n",
    "            temp_label = []\n",
    "            temp_masks = []\n",
    "            first_part = 0\n",
    "            for token, cue, label, mask in zip(sent, cues, labels, masks):\n",
    "                # (token, 1, 0, 1)\n",
    "                # find cues\n",
    "                if cue!=3:\n",
    "                    if first_part == 0:\n",
    "                        first_part = 1\n",
    "                        # add special token\n",
    "                        temp_sent.append(f'[unused{cue+1}]')\n",
    "                        # true_token_labels\n",
    "                        temp_masks.append(1)\n",
    "                        # scope label is 0 because it is a special token\n",
    "                        temp_label.append(0)\n",
    "                        # reconstruct the sentence\n",
    "                        temp_sent.append(token)\n",
    "                        # the actual token receives ZERO as true_token_label\n",
    "                        temp_masks.append(0)\n",
    "                        # scope label remains the same for the entire group of subwords\n",
    "                        temp_label.append(label)\n",
    "                        continue\n",
    "                    # the other parts of the subword:\n",
    "                    # add special token\n",
    "                    temp_sent.append(f'[unused{cue+1}]')\n",
    "                    # true_token_label is FALSE\n",
    "                    temp_masks.append(0)\n",
    "                    # scope_label stays the same for all the subwords. It is 0 because it's a cue.\n",
    "                    temp_label.append(0)\n",
    "                else:\n",
    "                    # if the token is not a cue\n",
    "                    first_part = 0\n",
    "                temp_masks.append(mask)\n",
    "                temp_sent.append(token)\n",
    "                temp_label.append(label)\n",
    "            final_sentences.append(temp_sent)\n",
    "            final_labels.append(temp_label)\n",
    "            final_masks.append(temp_masks)\n",
    "    else:\n",
    "        raise ValueError(\"Supported methods for scope detection are:\\nreplace\\naugment\")\n",
    "\n",
    "\n",
    "    return final_sentences, final_labels, final_masks         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YBuv0AgMeUfQ"
   },
   "outputs": [],
   "source": [
    "final_sentences, final_labels, final_masks = preprocess_data(scope_data, tokenizer)\n",
    "final_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Om-QZFrZrZa7"
   },
   "outputs": [],
   "source": [
    "# ##############################\n",
    "# collect vocab from data\n",
    "\n",
    "data_vocab = {}\n",
    "sentences = [t for s in final_sentences for t in s]\n",
    "\n",
    "for t in sentences:\n",
    "  if t not in data_vocab:\n",
    "    data_vocab[t] = 1\n",
    "  else:\n",
    "    data_vocab[t] += 1\n",
    "\n",
    "for i in sorted(data_vocab, key=data_vocab.get, reverse=True):\n",
    "  print(i, data_vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Co5Y0rngCuv"
   },
   "outputs": [],
   "source": [
    "input_ids = pad_sequences([[tokenizer._convert_token_to_id(word) for word in txt] for txt in final_sentences],\n",
    "                                      maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Za-sqdZhf72d"
   },
   "outputs": [],
   "source": [
    "tags = pad_sequences(final_labels, maxlen=MAX_LEN, value=0, padding=\"post\", dtype=\"long\", truncating=\"post\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e1RZ0loygk3Q"
   },
   "outputs": [],
   "source": [
    "finalest_masks = pad_sequences(final_masks, maxlen=MAX_LEN, value=0, padding='post', dtype='long', truncating='post').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgajZJAhgmfM"
   },
   "outputs": [],
   "source": [
    "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vWOeW1yJ0fCE"
   },
   "source": [
    "## INPUT for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1583337795020,
     "user": {
      "displayName": "Anastassia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64",
      "userId": "01489516284222648289"
     },
     "user_tz": -60
    },
    "id": "q6K420yQ0cF_",
    "outputId": "4bee7d70-3704-4c96-d838-c5974243554f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', \"'\", 'exam', '##en', 'end', '##os', '##co', '##pi', '##que', 'bro', '##nch', '##ique', '[unused3]', 'ne', 'rev', '##ele', '[unused3]', 'au', '[unused3]', '##cu', '[unused3]', '##ne', 'an', '##oma', '##lie', '.']\n",
      "26\n",
      "\n",
      "input_ids:  [1048, 1005, 11360, 2368, 2203, 2891, 3597, 8197, 4226, 22953, 12680, 7413, 4, 11265, 7065, 12260, 4, 8740, 4, 10841, 4, 2638, 2019, 9626, 8751, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "scope ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "true token masks:  [1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "attention masks:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "x = 3\n",
    "print(final_sentences[x])\n",
    "print(len(final_sentences[x]))\n",
    "print()\n",
    "print(\"input_ids: \", input_ids[x])\n",
    "print(\"scope ids: \", tags[x])\n",
    "print(\"true token masks: \", finalest_masks[x])\n",
    "print('attention masks: ', attention_masks[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OozMi4WWT0px"
   },
   "outputs": [],
   "source": [
    "def revert_ids_to_tokens(token_ids):\n",
    "  skip_special_tokens=True\n",
    "  clean_up_tokenization_spaces=True\n",
    "  special_ids = [1,2,3,4]\n",
    "\n",
    "  token_ids_todecode = [id for id in token_ids if id not in special_ids]\n",
    "  filtered_tokens = tokenizer.convert_ids_to_tokens(token_ids_todecode, skip_special_tokens=True)\n",
    "  # To avoid mixing byte-level and unicode for byte-level BPT\n",
    "  # we need to build string separatly for added tokens and byte-level tokens\n",
    "  # cf. https://github.com/huggingface/transformers/issues/1133\n",
    "  sub_texts = []\n",
    "  current_sub_text = []\n",
    "  for token in filtered_tokens:\n",
    "      if skip_special_tokens and token in tokenizer.all_special_ids:\n",
    "          continue\n",
    "      if token in tokenizer.added_tokens_encoder:\n",
    "          if current_sub_text:\n",
    "              sub_texts.append(tokenizer.convert_tokens_to_string(current_sub_text))\n",
    "              current_sub_text = []\n",
    "          sub_texts.append(token)\n",
    "      else:\n",
    "          current_sub_text.append(token)\n",
    "  if current_sub_text:\n",
    "      sub_texts.append(tokenizer.convert_tokens_to_string(current_sub_text))\n",
    "  text = \" \".join(sub_texts)\n",
    "\n",
    "  if clean_up_tokenization_spaces:\n",
    "      clean_text = tokenizer.clean_up_tokenization(text)\n",
    "      return clean_text\n",
    "  else:\n",
    "      return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1583339081700,
     "user": {
      "displayName": "Anastassia",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64",
      "userId": "01489516284222648289"
     },
     "user_tz": -60
    },
    "id": "glbWatx5W4uO",
    "outputId": "11c4183a-59d7-4b49-e4f8-a706fe20544b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l'examen endoscopique bronchique ne revele aucune anomalie.\n"
     ]
    }
   ],
   "source": [
    "print(revert_ids_to_tokens(token_ids_todecode))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "preprocess_sherlock.ipynb",
   "provenance": [
    {
     "file_id": "1nKZVSzdle4nc34g-XE4WmJ6KCTEC50ZA",
     "timestamp": 1579704151372
    },
    {
     "file_id": "1EMGz9atX_0rV906ynWj1lHGXhXuM9MvW",
     "timestamp": 1579508607571
    },
    {
     "file_id": "1jmoUcOE7lOwp-fqj6wjtHZpe92ob3Xom",
     "timestamp": 1579171866963
    },
    {
     "file_id": "https://github.com/adityak6798/Transformers-For-Negation-and-Speculation/blob/master/Transformers_for_Negation_and_Speculation.ipynb",
     "timestamp": 1578674765715
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "3fd5aa1c48954f30ad7d025ea449d930": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "741977bbaaf349009cf63766f7d935c7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a80e0f025b649258b3cd555a2e256f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e83950142d5543c3ac2d8143c1007a45",
       "IPY_MODEL_bdd92ea2c9fb4699b819ba23ee5d06a0"
      ],
      "layout": "IPY_MODEL_741977bbaaf349009cf63766f7d935c7"
     }
    },
    "7e597852111c4ad8ba91f6e1e969a645": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a4cfe1b71404c5387eba53cd6cdab95": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7fe35fb1f414f849de09bc45a272697": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bdd92ea2c9fb4699b819ba23ee5d06a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e597852111c4ad8ba91f6e1e969a645",
      "placeholder": "​",
      "style": "IPY_MODEL_a7fe35fb1f414f849de09bc45a272697",
      "value": "100% 232k/232k [00:00&lt;00:00, 2.53MB/s]"
     }
    },
    "e83950142d5543c3ac2d8143c1007a45": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a4cfe1b71404c5387eba53cd6cdab95",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3fd5aa1c48954f30ad7d025ea449d930",
      "value": 231508
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
