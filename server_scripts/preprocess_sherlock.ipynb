{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocess_sherlock.ipynb","provenance":[{"file_id":"1nKZVSzdle4nc34g-XE4WmJ6KCTEC50ZA","timestamp":1579704151372},{"file_id":"1EMGz9atX_0rV906ynWj1lHGXhXuM9MvW","timestamp":1579508607571},{"file_id":"1jmoUcOE7lOwp-fqj6wjtHZpe92ob3Xom","timestamp":1579171866963},{"file_id":"https://github.com/adityak6798/Transformers-For-Negation-and-Speculation/blob/master/Transformers_for_Negation_and_Speculation.ipynb","timestamp":1578674765715}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7a80e0f025b649258b3cd555a2e256f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_741977bbaaf349009cf63766f7d935c7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e83950142d5543c3ac2d8143c1007a45","IPY_MODEL_bdd92ea2c9fb4699b819ba23ee5d06a0"]}},"741977bbaaf349009cf63766f7d935c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e83950142d5543c3ac2d8143c1007a45":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3fd5aa1c48954f30ad7d025ea449d930","_dom_classes":[],"description":"Downloading","_model_name":"IntProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8a4cfe1b71404c5387eba53cd6cdab95"}},"bdd92ea2c9fb4699b819ba23ee5d06a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a7fe35fb1f414f849de09bc45a272697","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 232k/232k [00:00&lt;00:00, 2.53MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7e597852111c4ad8ba91f6e1e969a645"}},"3fd5aa1c48954f30ad7d025ea449d930":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8a4cfe1b71404c5387eba53cd6cdab95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a7fe35fb1f414f849de09bc45a272697":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7e597852111c4ad8ba91f6e1e969a645":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"hcsQGMrRGJsH","colab_type":"code","colab":{}},"source":["# Installations needed\n","!pip install transformers\n","!pip install knockknock"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PenMagTOWnWw","colab_type":"code","colab":{}},"source":["!pip install -U ipykernel"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mGTg8YDGGbCy","colab_type":"code","colab":{}},"source":["# Imports needed\n","import os, re, torch, html, tempfile, copy, json, math, shutil, tarfile, tempfile, sys, random, pickle\n","from keras.preprocessing.sequence import pad_sequences\n","from transformers import BertTokenizer, BertConfig, BertModel, WordpieceTokenizer\n","from transformers.file_utils import cached_path\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tq0WavgDIGJy","colab_type":"text"},"source":["Upload the datasets to Google Drive. \n","This allows access to your Google Drive from this notebook."]},{"cell_type":"code","metadata":{"id":"h7_IpJcCHOGK","colab_type":"code","outputId":"686dfb3b-8db8-4955-82e9-c46101eaf236","executionInfo":{"status":"ok","timestamp":1583333724410,"user_tz":-60,"elapsed":13238,"user":{"displayName":"Anastassia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64","userId":"01489516284222648289"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0BuBr5yWJ1Cj","colab_type":"text"},"source":["# Class Definitions:\n","## Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"gQQnT6Kydye8","colab_type":"code","colab":{}},"source":["sherlock_data = '/content/gdrive/My Drive/multilingual_BERT_negations/data/CAS_sherlock_full.txt'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xe1kG3IZKBrD","colab_type":"code","colab":{}},"source":["def starsem(f_path, cue_sents_only=False, frac_no_cue_sents = 1.0):\n","    raw_data = open(f_path)\n","    sentence = []\n","    labels = []\n","    label = []\n","    scope_sents = []\n","    data_scope = []\n","    scope = []\n","    scope_cues = []\n","    # list of lists of all sentences\n","    data = []\n","    cue_only_data = []\n","    \n","    for line in raw_data:\n","        # print(line)\n","        label = []\n","        sentence = []\n","        tokens = line.strip().split()\n","        if len(tokens)==8: #This line has no cues\n","                print(tokens)\n","                # append the word\n","                sentence.append(tokens[3])\n","                label.append(3) #Not a cue\n","                for line in raw_data:\n","                    tokens = line.strip().split()\n","                    if len(tokens)==0:\n","                        break\n","                    else:\n","                        sentence.append(tokens[3])\n","                        label.append(3)\n","                cue_only_data.append([sentence, label])\n","                \n","        # elif len(tokens) == 1:\n","          # print(tokens)  \n","        else: #The line has 1 or more cues\n","            num_cues = (len(tokens)-7)//3\n","            #cue_count+=num_cues\n","            scope = [[] for i in range(num_cues)]\n","            label = [[],[]] #First list is the real labels, second list is to modify if it is a multi-word cue.\n","            label[0].append(3) #Generally not a cue, if it is will be set ahead.\n","            label[1].append(-1) #Since not a cue, for now.\n","            # print(label)\n","            for i in range(num_cues):\n","                if tokens[7+3*i] != '_': #Cue field is active\n","                    if tokens[8+3*i] != '_': #Check for affix\n","                        label[0][-1] = 0 #Affix\n","                        # affix_list.append(tokens[7+3*i])\n","                        label[1][-1] = i #Cue number\n","                        #sentence.append(tokens[7+3*i])\n","                        #new_word = '##'+tokens[8+3*i]\n","                    else:\n","                        label[0][-1] = 1 #Maybe a normal or multiword cue. The next few words will determine which.\n","                        label[1][-1] = i #Which cue field, for multiword cue altering.\n","                        \n","                if tokens[8+3*i] != '_':\n","                    scope[i].append(1)\n","                else:\n","                    scope[i].append(0)\n","            sentence.append(tokens[3])\n","            for line in raw_data:\n","                tokens = line.strip().split()\n","                if len(tokens)==0:\n","                    break\n","                else:\n","                    sentence.append(tokens[3])\n","                    label[0].append(3) #Generally not a cue, if it is will be set ahead.\n","                    label[1].append(-1) #Since not a cue, for now.   \n","                    for i in range(num_cues):\n","                        if tokens[7+3*i] != '_': #Cue field is active\n","                            if tokens[8+3*i] != '_': #Check for affix\n","                                label[0][-1] = 0 #Affix\n","                                label[1][-1] = i #Cue number\n","                            else:\n","                                label[0][-1] = 1 #Maybe a normal or multiword cue. The next few words will determine which.\n","                                label[1][-1] = i #Which cue field, for multiword cue altering.\n","                        if tokens[8+3*i] != '_':\n","                            scope[i].append(1)\n","                        else:\n","                            scope[i].append(0)\n","            for i in range(num_cues):\n","                indices = [index for index,j in enumerate(label[1]) if i==j]\n","                count = len(indices)\n","                if count>1:\n","                    for j in indices:\n","                        label[0][j] = 2\n","            for i in range(num_cues):\n","                sc = []\n","                for a,b in zip(label[0],label[1]):\n","                    if i==b:\n","                        sc.append(a)\n","                    else:\n","                        sc.append(3)\n","                scope_cues.append(sc)\n","                scope_sents.append(sentence)\n","                data_scope.append(scope[i])\n","            labels.append(label[0])\n","            # print(labels)\n","            data.append(sentence)\n","    # print(type(data))\n","    # print(data)\n","    cue_only_samples = random.sample(cue_only_data, k=int(frac_no_cue_sents*len(cue_only_data)))\n","    cue_only_sents = [i[0] for i in cue_only_samples]\n","    cue_only_cues = [i[1] for i in cue_only_samples]\n","    starsem_cues = (data+cue_only_sents,labels+cue_only_cues)\n","    starsem_scopes = (scope_sents, scope_cues, data_scope)\n","    return [starsem_cues, starsem_scopes]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7flcwFZKSF4L","colab_type":"code","colab":{}},"source":["starsem_cues, starsem_scopes = starsem(sherlock_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbDnVo7RddsM","colab_type":"code","colab":{}},"source":["scope_sents, scope_cues, data_scope = starsem_scopes\n","scope_data = zip(scope_sents, scope_cues, data_scope)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I0whM2jJlL23","colab_type":"code","outputId":"7eea4c90-7122-47d4-eda8-84528857edec","executionInfo":{"status":"ok","timestamp":1583337746292,"user_tz":-60,"elapsed":632,"user":{"displayName":"Anastassia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64","userId":"01489516284222648289"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["print(scope_sents[3])\n","print(scope_cues[3])\n","print('Scope labels:', data_scope[3])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[\"l'\", 'examen', 'endoscopique', 'bronchique', 'ne', 'révèle', 'aucune', 'anomalie', '.']\n","[3, 3, 3, 3, 2, 3, 2, 3, 3]\n","Scope labels: [0, 0, 0, 0, 0, 1, 0, 1, 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VxWHdxyaNs79","colab_type":"text"},"source":["Tokenize with BERT and convert the input to tensors"]},{"cell_type":"code","metadata":{"id":"0--9vurTqgIS","colab_type":"code","colab":{}},"source":["SCOPE_MODEL = 'bert-base-uncased'\n","# if 'bert-base-multilingual-cased' do_lower_case=False\n","do_lower_case = True\n","MAX_LEN = 128\n","method = 'augment'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LlN2xBhOIXwB","colab_type":"code","colab":{}},"source":["device = torch.device(\"cuda\")\n","n_gpu = torch.cuda.device_count()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hr9AXN9fMI2R","colab_type":"code","outputId":"0a015040-fca5-4e41-f8b9-78952871e14a","executionInfo":{"status":"ok","timestamp":1583337423431,"user_tz":-60,"elapsed":1003,"user":{"displayName":"Anastassia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64","userId":"01489516284222648289"}},"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["7a80e0f025b649258b3cd555a2e256f2","741977bbaaf349009cf63766f7d935c7","e83950142d5543c3ac2d8143c1007a45","bdd92ea2c9fb4699b819ba23ee5d06a0","3fd5aa1c48954f30ad7d025ea449d930","8a4cfe1b71404c5387eba53cd6cdab95","a7fe35fb1f414f849de09bc45a272697","7e597852111c4ad8ba91f6e1e969a645"]}},"source":["tokenizer = BertTokenizer.from_pretrained(SCOPE_MODEL, do_lower_case=do_lower_case, cache_dir='bert_tokenizer')"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a80e0f025b649258b3cd555a2e256f2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zjGe5nobpnSY","colab_type":"code","colab":{}},"source":["# ####################################\n","# collect example of BERT tokenization\n","sents = [\" \".join([s for s in sent]) for sent in scope_sents]\n","[sent.lower() for sent in sents]\n","\n","for word in sents[1].split():\n","  print(word)\n","  subwords = tokenizer.tokenize(word)\n","  print(word, subwords)\n","\n","# au\n","# au ['au']\n","# niveau\n","# niveau ['niveau']\n","# thoracique\n","# thoracique ['th', '##ora', '##ci', '##que']\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEQ1qFrgdQb2","colab_type":"code","colab":{}},"source":["def preprocess_data(zipped, tokenizer_obj):\n","    \n","    zipped = list(zipped)\n","    dl_sents = [item[0] for item in zipped]\n","    dl_cues = [item[1] for item in zipped]\n","    dl_scopes = [item[2] for item in zipped]\n","    \n","    # print(dl_sents[1])\n","    sentences = [\" \".join([s for s in sent]) for sent in dl_sents]\n","    mytexts = []\n","    mylabels = []\n","    mycues = []\n","    mymasks = []\n","    if do_lower_case == True:\n","        sentences_clean = [sent.lower() for sent in sentences]\n","    else:\n","        sentences_clean = sentences\n","    \n","    for sent, tags, cues in zip(sentences_clean, dl_scopes, dl_cues):\n","        new_tags = []\n","        new_text = []\n","        new_cues = []\n","        new_masks = []\n","        for word, tag, cue in zip(sent.split(), tags, cues):\n","            sub_words = tokenizer_obj._tokenize(word)\n","\n","            # update tags and labels for subwords\n","            for count, sub_word in enumerate(sub_words):\n","                # the first subword is the true token\n","                mask = 1\n","                if count > 0:\n","                    # all the other subwords that start with # receive a mask 0\n","                    # these will be the true_token_masks\n","                    mask = 0\n","\n","                # start collecting masks for true and not true tokens\n","                new_masks.append(mask)\n","\n","                # tags(scope) and cue labels stay the same for all the subwords\n","                new_tags.append(tag)\n","                new_cues.append(cue)\n","                new_text.append(sub_word)\n","        # true and not true tokens\n","        mymasks.append(new_masks)\n","        mytexts.append(new_text)\n","        mylabels.append(new_tags)\n","        mycues.append(new_cues)\n","\n","    # start lists of final inputs\n","    final_sentences = []\n","    final_labels = []\n","    final_masks = []\n","    if method == 'replace':\n","        for sent,cues in zip(mytexts, mycues):\n","            temp_sent = []\n","            for token,cue in zip(sent,cues):\n","                if cue==3:\n","                    temp_sent.append(token)\n","                else:\n","                    temp_sent.append(f'[unused{cue+1}]')\n","            final_sentences.append(temp_sent)\n","        final_labels = mylabels\n","        final_masks = mymasks\n","    elif method == 'augment':\n","        # mylabels are scope tags\n","        # input is already subword-tokenized by BERT\n","        for sent, cues, labels, masks in zip(mytexts, mycues, mylabels, mymasks):\n","          \n","            temp_sent = []\n","            temp_label = []\n","            temp_masks = []\n","            first_part = 0\n","            for token, cue, label, mask in zip(sent, cues, labels, masks):\n","                # (token, 1, 0, 1)\n","                # find cues\n","                if cue!=3:\n","                    if first_part == 0:\n","                        first_part = 1\n","                        # add special token\n","                        temp_sent.append(f'[unused{cue+1}]')\n","                        # true_token_labels\n","                        temp_masks.append(1)\n","                        # scope label is 0 because it is a special token\n","                        temp_label.append(0)\n","                        # reconstruct the sentence\n","                        temp_sent.append(token)\n","                        # the actual token receives ZERO as true_token_label\n","                        temp_masks.append(0)\n","                        # scope label remains the same for the entire group of subwords\n","                        temp_label.append(label)\n","                        continue\n","                    # the other parts of the subword:\n","                    # add special token\n","                    temp_sent.append(f'[unused{cue+1}]')\n","                    # true_token_label is FALSE\n","                    temp_masks.append(0)\n","                    # scope_label stays the same for all the subwords. It is 0 because it's a cue.\n","                    temp_label.append(0)\n","                else:\n","                    # if the token is not a cue\n","                    first_part = 0\n","                temp_masks.append(mask)\n","                temp_sent.append(token)\n","                temp_label.append(label)\n","            final_sentences.append(temp_sent)\n","            final_labels.append(temp_label)\n","            final_masks.append(temp_masks)\n","    else:\n","        raise ValueError(\"Supported methods for scope detection are:\\nreplace\\naugment\")\n","\n","\n","    return final_sentences, final_labels, final_masks         "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBuv0AgMeUfQ","colab_type":"code","colab":{}},"source":["final_sentences, final_labels, final_masks = preprocess_data(scope_data, tokenizer)\n","final_sentences"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Om-QZFrZrZa7","colab_type":"code","colab":{}},"source":["# ##############################\n","# collect vocab from data\n","\n","data_vocab = {}\n","sentences = [t for s in final_sentences for t in s]\n","\n","for t in sentences:\n","  if t not in data_vocab:\n","    data_vocab[t] = 1\n","  else:\n","    data_vocab[t] += 1\n","\n","for i in sorted(data_vocab, key=data_vocab.get, reverse=True):\n","  print(i, data_vocab[i])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Co5Y0rngCuv","colab_type":"code","colab":{}},"source":["input_ids = pad_sequences([[tokenizer._convert_token_to_id(word) for word in txt] for txt in final_sentences],\n","                                      maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\").tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Za-sqdZhf72d","colab_type":"code","colab":{}},"source":["tags = pad_sequences(final_labels, maxlen=MAX_LEN, value=0, padding=\"post\", dtype=\"long\", truncating=\"post\").tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e1RZ0loygk3Q","colab_type":"code","colab":{}},"source":["finalest_masks = pad_sequences(final_masks, maxlen=MAX_LEN, value=0, padding='post', dtype='long', truncating='post').tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jgajZJAhgmfM","colab_type":"code","colab":{}},"source":["attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vWOeW1yJ0fCE","colab_type":"text"},"source":["## INPUT for BERT"]},{"cell_type":"code","metadata":{"id":"q6K420yQ0cF_","colab_type":"code","outputId":"4bee7d70-3704-4c96-d838-c5974243554f","executionInfo":{"status":"ok","timestamp":1583337795020,"user_tz":-60,"elapsed":553,"user":{"displayName":"Anastassia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64","userId":"01489516284222648289"}},"colab":{"base_uri":"https://localhost:8080/","height":161}},"source":["x = 3\n","print(final_sentences[x])\n","print(len(final_sentences[x]))\n","print()\n","print(\"input_ids: \", input_ids[x])\n","print(\"scope ids: \", tags[x])\n","print(\"true token masks: \", finalest_masks[x])\n","print('attention masks: ', attention_masks[x])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['l', \"'\", 'exam', '##en', 'end', '##os', '##co', '##pi', '##que', 'bro', '##nch', '##ique', '[unused3]', 'ne', 'rev', '##ele', '[unused3]', 'au', '[unused3]', '##cu', '[unused3]', '##ne', 'an', '##oma', '##lie', '.']\n","26\n","\n","input_ids:  [1048, 1005, 11360, 2368, 2203, 2891, 3597, 8197, 4226, 22953, 12680, 7413, 4, 11265, 7065, 12260, 4, 8740, 4, 10841, 4, 2638, 2019, 9626, 8751, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","scope ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","true token masks:  [1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","attention masks:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OozMi4WWT0px","colab_type":"code","colab":{}},"source":["def revert_ids_to_tokens(token_ids):\n","  skip_special_tokens=True\n","  clean_up_tokenization_spaces=True\n","  special_ids = [1,2,3,4]\n","\n","  token_ids_todecode = [id for id in token_ids if id not in special_ids]\n","  filtered_tokens = tokenizer.convert_ids_to_tokens(token_ids_todecode, skip_special_tokens=True)\n","  # To avoid mixing byte-level and unicode for byte-level BPT\n","  # we need to build string separatly for added tokens and byte-level tokens\n","  # cf. https://github.com/huggingface/transformers/issues/1133\n","  sub_texts = []\n","  current_sub_text = []\n","  for token in filtered_tokens:\n","      if skip_special_tokens and token in tokenizer.all_special_ids:\n","          continue\n","      if token in tokenizer.added_tokens_encoder:\n","          if current_sub_text:\n","              sub_texts.append(tokenizer.convert_tokens_to_string(current_sub_text))\n","              current_sub_text = []\n","          sub_texts.append(token)\n","      else:\n","          current_sub_text.append(token)\n","  if current_sub_text:\n","      sub_texts.append(tokenizer.convert_tokens_to_string(current_sub_text))\n","  text = \" \".join(sub_texts)\n","\n","  if clean_up_tokenization_spaces:\n","      clean_text = tokenizer.clean_up_tokenization(text)\n","      return clean_text\n","  else:\n","      return text\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"glbWatx5W4uO","colab_type":"code","outputId":"11c4183a-59d7-4b49-e4f8-a706fe20544b","executionInfo":{"status":"ok","timestamp":1583339081700,"user_tz":-60,"elapsed":801,"user":{"displayName":"Anastassia","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhjwLyI2Zod0wxpLXpagChNSa34Jfk0OcGAsMuo9w=s64","userId":"01489516284222648289"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(revert_ids_to_tokens(token_ids_todecode))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["l'examen endoscopique bronchique ne revele aucune anomalie.\n"],"name":"stdout"}]}]}