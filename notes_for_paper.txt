# to run

1. prepare for annotation
conda activate prodigy
prodigy my.manual SUPERANNO_PROJEKT.sqlite blank:ru data4prodigy.jsonl --label Cue,Scope -F my_prodigy_manual.py




ANNOTATIONS:

71	hoteles/no_2_20.tbf.xml
A mi no me gustó , deberian hacer una revisión y adecuar la calidad de el hotel a las estrellas .
no (cues)
A mi me gustó (scope)

for russian: у нас не было теории

No existe ninguna otra de llegar a el aeropuerto que no sea en taxi o en su transfer .
No ninguna otra que no sea (cues)
existe de llegar a el aeropuerto (scope)

No nos dieron ninguna facilidad.
No ninguna (cues)
nos dieron facilidad (scope)

The value of <discid> is represented both numerically (1 in the example below), which indicates the numerical order of the discontinuous negative elements in that negative structure, and as a letter ‘n’ and ‘c’, where ‘n’ and ‘c’ indicate the first (nucleus or core) and second element of the negation respectively.



Spanish:
take out noneg
Y bueno, casi no me quieren ni aceptar la tarifa web, me tuve que poner farruca, por no decir que al principio la chica incluso dudaba si tendría habitaciones para nosotros.
And well, they almost don't even want me to accept the web rate, I had to get farruca, not to mention that at first the girl even doubted if she would have rooms for us.
"por no decir que" (not to mention) is marked as negexp but neg_structure value is "NONEG"


In SFU Review the phrase "not to mention" is marked for negation (for example in MUSIC/no11done.xml): "not" is a cue with type="negation" and the rest is scope.




CAS: 945 negation events turned into sentences
before I had 914!

ESSAI: 1079 neg sentences from my script.
       1064 on the site https://clementdalloux.fr/?page_id=28


extract test text from Sherlcok.
Apparently, I use only train and dev data. My test files remained unused.

133 is the len of Cardboard json


I extracted intermediate representations of annotations from each corpus.
I unified annotations a little:
        - I excluded cues from scope in BIOSCOPE.
        - I removed morphological annotations in Sherlock.




These will have to be turned into NegBERT's intermediate annotation:

"Сам я за время службы в Индии привык переносить жару лучше, чем холод, и тридцать три градуса выше нуля не особенно меня тяготили. "
"spans":[{"start":73,"end":103,"token_start":16,"token_end":20,"label":"Scope"},
        {"start":104,"end":106,"token_start":21,"token_end":21,"label":"Cue"},
        {"start":107,"end":129,"token_start":22,"token_end":24,"label":"Scope"}]


Idea: involve Michi?

multiple languages
interpreting the model?
	- saliency: gradient-based, erasure-based, etc.
	- attention
	- perturbation
use an lstm in fairseq as baseline?
Linguistic or syntactic out-of-distribution generalization (leftward scope?) => to observe variability. (BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance. R. Thomas Mccoy, Junghyun Min and Tal Linzen.)

LTH (Lottery Ticket Hypothesis): pruning BERT? (When BERT Plays the Lottery, All Tickets Are Winning!)
