<!-- RRGen Human Evaluation Insturctions -->

<div>
    <h1>RRGen Human Evaluation: HELP!</h1>

    <p>Each annotation example consists of a REVIEW text and a generated RESPONSE text.</p>

    <p>For each REVIEW text, there are three different
    RESPONSE texts which are randomised and shown in succession to improve
    annotator productivity. 
    <br />
    For example, REVIEW A has:<br/>
    <ul>
        <li>RESPONSE from model B</li>
        <li>RESPONSE from model A</li>
        <li>RESPONSE from model C</li>
    </ul>
    <br />
    The purpose of the annotation schema is to compare
    outputs from different models against each other. We collect the same
    evaluation criteria for each model output text.
    </p>

    <h2>Evaluation</h2>

    <p>
        For each RESPONSE text, we collect two types of
        annotations.
    </p>
    <p>
        First, we collect scores on likert scales of 1 to 7 for the following evaluation criteria:
    </p>
    <ul>
        <li><strong>SEMANTIC RELEVANCE</strong>: indicates the degree to which semantic content of
        the REVIEW is reflected/addressed in the RESPONSE (*)</li>
        <li><strong>FLUENCY</strong>: indicates the degree to which the RESPONSE text is
        grammatically fluent</li>
        <li><strong>SENTIMENT APPROPRIATENESS</strong>: indicates the degree to which the RESPONSE
        text accurately reflects the sentiment conveyed in the REVIEW
        (e.g. positive vs. negative sentiment)</li>
        <li><strong>DOMAIN ACCURACY</strong>: indicates the degree to which the RESPONSE text
        accurately reflects the domain of the REVIEW
        (e.g. Hotel vs. Restaurant)</li>
    </ul>
    <p>
        A score of 1 is the lowest, while 7 is the
        highest.<br/><br/>
        1 = Not at all<br/>
        2 = Slightly<br/>
        3 = More than slightly<br/>
        4 = Partially<br/>
        5 = More than partially<br/>
        6 = Almost completely<br/>
        7 = Completely<br/>        
    </p>
    <p>
        Note, 3 and 5 are reserved for difficult/borderline
        cases and should be used sparingly.
    </p>
    <p>
        Second, we collect <strong>ACCEPT / REJECT</strong>
        information that tells us whether or not the
        RESPONSE text is suitable.<br/>
        <strong>* NOTE</strong>: if the REVIEW text does not
        contain any semantically important information or is
        deemed unsuitable given the REVIEW text, the
        example should be ignored completely.
    </p>
    <p>
        In addition, it is also possible to highlight
        passages in the RESPONSE text that are particularly
        relevant or address explicitly semantic content of the REVIEW text.
    </p>
</div>

